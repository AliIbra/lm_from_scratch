{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# imports and data\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Upload your files interactively\n",
        "uploaded = files.upload()\n",
        "# After upload, the files will be in the current working directory\n",
        "TRAIN_FILE = \"Shakespeare_clean_train.txt\"\n",
        "VALIDATION_FILE = \"Shakespeare_clean_valid.txt\"\n",
        "TEST_FILE = \"Shakespeare_clean_test.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "0JvH8-ZThC_R",
        "outputId": "5e973014-8be0-4829-a133-2712f9ca686b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-507dff81-7c66-45ba-983f-da1a952c84b0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-507dff81-7c66-45ba-983f-da1a952c84b0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving bpe_encoder.pkl to bpe_encoder.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "srxrFvUYgjAj"
      },
      "outputs": [],
      "source": [
        "# load BPE Encoder\n",
        "def load_bpe_encoder(path='bpe_encoder.pkl'):\n",
        "    with open(path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "# tokenize Text using BPE\n",
        "def bpe_tokenize_word(word, merges):\n",
        "    tokens = list(word) + ['</w>']\n",
        "    for pair in merges:\n",
        "        replacement = ''.join(pair)\n",
        "        i = 0\n",
        "        new_tokens = []\n",
        "        while i < len(tokens):\n",
        "            if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == pair:\n",
        "                new_tokens.append(replacement)\n",
        "                i += 2\n",
        "            else:\n",
        "                new_tokens.append(tokens[i])\n",
        "                i += 1\n",
        "        tokens = new_tokens\n",
        "    return tokens\n",
        "\n",
        "def tokenize_text(text, merges):\n",
        "    tokens = []\n",
        "    for word in text.strip().split():\n",
        "        tokens.extend(bpe_tokenize_word(word.lower(), merges))\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocab and dataset\n",
        "def build_vocab(tokens):\n",
        "    vocab = sorted(set(tokens))\n",
        "    token_to_id = {token: i for i, token in enumerate(vocab)}\n",
        "    id_to_token = {i: token for token, i in token_to_id.items()}\n",
        "    return token_to_id, id_to_token\n",
        "\n",
        "def create_trigrams(token_ids):\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    for i in range(len(token_ids) - 2):\n",
        "        inputs.append([token_ids[i], token_ids[i+1]])\n",
        "        targets.append(token_ids[i+2])\n",
        "    return np.array(inputs), np.array(targets)\n"
      ],
      "metadata": {
        "id": "QuWANQQRgp5f"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trigram feedforward model\n",
        "class NeuralTrigramModel:\n",
        "    def __init__(self, vocab_size, embedding_dim=32, hidden_dim=128):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Initialize weights\n",
        "        self.embeddings = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
        "        self.W1 = np.random.randn(2 * embedding_dim, hidden_dim) * 0.01\n",
        "        self.b1 = np.zeros((hidden_dim,))\n",
        "        self.W2 = np.random.randn(hidden_dim, vocab_size) * 0.01\n",
        "        self.b2 = np.zeros((vocab_size,))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, 2)\n",
        "        emb = self.embeddings[x]  # (batch_size, 2, emb_dim)\n",
        "        emb = emb.reshape(x.shape[0], -1)  # flatten to (batch_size, 2*emb_dim)\n",
        "        h = np.tanh(emb @ self.W1 + self.b1)  # (batch_size, hidden_dim)\n",
        "        logits = h @ self.W2 + self.b2        # (batch_size, vocab_size)\n",
        "        return logits\n",
        "\n",
        "    def compute_loss(self, logits, targets):\n",
        "        # Cross entropy loss\n",
        "        probs = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
        "        probs /= probs.sum(axis=1, keepdims=True)\n",
        "        loss = -np.log(probs[range(len(targets)), targets]).mean()\n",
        "        return loss, probs\n",
        "\n",
        "    def backward(self, x, targets, probs, lr=0.1):\n",
        "        batch_size = x.shape[0]\n",
        "        one_hot = np.zeros_like(probs)\n",
        "        one_hot[np.arange(batch_size), targets] = 1\n",
        "        dlogits = (probs - one_hot) / batch_size\n",
        "\n",
        "        # Forward pass parts\n",
        "        emb = self.embeddings[x].reshape(batch_size, -1)\n",
        "        h = np.tanh(emb @ self.W1 + self.b1)\n",
        "\n",
        "        # Gradients\n",
        "        dW2 = h.T @ dlogits\n",
        "        db2 = dlogits.sum(axis=0)\n",
        "\n",
        "        dh = dlogits @ self.W2.T\n",
        "        dh_raw = dh * (1 - h**2)\n",
        "\n",
        "        dW1 = emb.T @ dh_raw\n",
        "        db1 = dh_raw.sum(axis=0)\n",
        "\n",
        "        demb = dh_raw @ self.W1.T\n",
        "        demb = demb.reshape(batch_size, 2, self.embedding_dim)\n",
        "\n",
        "        # Update embeddings\n",
        "        for i in range(2):\n",
        "            np.add.at(self.embeddings, x[:, i], -lr * demb[:, i])\n",
        "\n",
        "        # Update weights\n",
        "        self.W1 -= lr * dW1\n",
        "        self.b1 -= lr * db1\n",
        "        self.W2 -= lr * dW2\n",
        "        self.b2 -= lr * db2\n"
      ],
      "metadata": {
        "id": "1LzDJ1Upgqy7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model save and load\n",
        "def save(self, path='neural_trigram_model.pkl'):\n",
        "    with open(path, 'wb') as f:\n",
        "        pickle.dump({\n",
        "            'embeddings': self.embeddings,\n",
        "            'W1': self.W1,\n",
        "            'b1': self.b1,\n",
        "            'W2': self.W2,\n",
        "            'b2': self.b2\n",
        "        }, f)\n",
        "    print(f\"✅ Neural model saved to: {path}\")\n",
        "\n",
        "def load(self, path='neural_trigram_model.pkl'):\n",
        "    with open(path, 'rb') as f:\n",
        "        params = pickle.load(f)\n",
        "        self.embeddings = params['embeddings']\n",
        "        self.W1 = params['W1']\n",
        "        self.b1 = params['b1']\n",
        "        self.W2 = params['W2']\n",
        "        self.b2 = params['b2']\n",
        "    print(f\"✅ Neural model loaded from: {path}\")\n"
      ],
      "metadata": {
        "id": "K6GliHzDgt76"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preplexity score\n",
        "def compute_perplexity(model, tokens, token_to_id, n=3):\n",
        "    token_ids = [token_to_id.get(t, 0) for t in tokens]\n",
        "    total_log_prob = 0\n",
        "    count = 0\n",
        "\n",
        "    for i in range(len(token_ids) - 2):\n",
        "        context = token_ids[i:i+2]\n",
        "        target = token_ids[i+2]\n",
        "\n",
        "        x = np.array([context])\n",
        "        logits = model.forward(x)\n",
        "        probs = np.exp(logits - np.max(logits))\n",
        "        probs /= probs.sum()\n",
        "\n",
        "        prob = probs[0][target]\n",
        "        total_log_prob += np.log(prob + 1e-10)  # add epsilon to avoid log(0)\n",
        "        count += 1\n",
        "\n",
        "    perplexity = np.exp(-total_log_prob / count) if count > 0 else float('inf')\n",
        "    return perplexity\n"
      ],
      "metadata": {
        "id": "lsUo764Sgv_t"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "def train_model(model, inputs, targets, epochs=5, batch_size=512, lr=0.1):\n",
        "    for epoch in range(epochs):\n",
        "        permutation = np.random.permutation(len(inputs))\n",
        "        inputs_shuffled = inputs[permutation]\n",
        "        targets_shuffled = targets[permutation]\n",
        "\n",
        "        total_loss = 0.0\n",
        "        for i in range(0, len(inputs), batch_size):\n",
        "            x_batch = inputs_shuffled[i:i+batch_size]\n",
        "            y_batch = targets_shuffled[i:i+batch_size]\n",
        "\n",
        "            logits = model.forward(x_batch)\n",
        "            loss, probs = model.compute_loss(logits, y_batch)\n",
        "            model.backward(x_batch, y_batch, probs, lr)\n",
        "            total_loss += loss * len(x_batch)\n",
        "\n",
        "        avg_loss = total_loss / len(inputs)\n",
        "        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "481pI3RRgyIA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text generation\n",
        "def generate_text(model, token_to_id, id_to_token, start_tokens=['the'], length=20):\n",
        "    context = start_tokens[:2]\n",
        "    result = context.copy()\n",
        "\n",
        "    for _ in range(length):\n",
        "        if len(context) < 2:\n",
        "            context = ['<unk>'] * (2 - len(context)) + context\n",
        "        x = np.array([[token_to_id.get(context[-2], 0), token_to_id.get(context[-1], 0)]])\n",
        "        logits = model.forward(x)\n",
        "        probs = np.exp(logits - np.max(logits))\n",
        "        probs /= probs.sum()\n",
        "        next_token_id = np.random.choice(len(probs[0]), p=probs[0])\n",
        "        next_token = id_to_token[next_token_id]\n",
        "        result.append(next_token)\n",
        "        context.append(next_token)\n",
        "\n",
        "    return \" \".join(t.replace('</w>', '') for t in result)\n"
      ],
      "metadata": {
        "id": "0VTQtGhDg0S_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load text\n",
        "with open(TRAIN_FILE, 'r', encoding='utf-8') as f:\n",
        "    train_text = f.read().lower()\n",
        "\n",
        "#load bpe encoder\n",
        "merges = load_bpe_encoder()\n",
        "\n",
        "tokens = tokenize_text(train_text, merges)\n",
        "token_to_id, id_to_token = build_vocab(tokens)\n",
        "token_ids = [token_to_id[t] for t in tokens]\n",
        "\n",
        "# Prepare data\n",
        "inputs, targets = create_trigrams(token_ids)\n",
        "\n",
        "# Initialize and train model\n",
        "model = NeuralTrigramModel(vocab_size=len(token_to_id))\n",
        "train_model(model, inputs, targets, epochs=100, lr=0.05)\n",
        "\n",
        "# Generate text\n",
        "print(\"\\nGenerated Text:\")\n",
        "print(generate_text(model, token_to_id, id_to_token, start_tokens=['the']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAE7TX9eg2nK",
        "outputId": "4860dd76-bcf1-4647-b630-1e54471a674d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss = 8.6497\n",
            "Epoch 2: Loss = 8.5778\n",
            "Epoch 3: Loss = 8.4671\n",
            "Epoch 4: Loss = 8.1451\n",
            "Epoch 5: Loss = 7.7698\n",
            "Epoch 6: Loss = 7.5780\n",
            "Epoch 7: Loss = 7.4707\n",
            "Epoch 8: Loss = 7.4028\n",
            "Epoch 9: Loss = 7.3591\n",
            "Epoch 10: Loss = 7.3301\n",
            "Epoch 11: Loss = 7.3099\n",
            "Epoch 12: Loss = 7.2953\n",
            "Epoch 13: Loss = 7.2846\n",
            "Epoch 14: Loss = 7.2763\n",
            "Epoch 15: Loss = 7.2700\n",
            "Epoch 16: Loss = 7.2651\n",
            "Epoch 17: Loss = 7.2611\n",
            "Epoch 18: Loss = 7.2579\n",
            "Epoch 19: Loss = 7.2553\n",
            "Epoch 20: Loss = 7.2531\n",
            "Epoch 21: Loss = 7.2514\n",
            "Epoch 22: Loss = 7.2498\n",
            "Epoch 23: Loss = 7.2485\n",
            "Epoch 24: Loss = 7.2474\n",
            "Epoch 25: Loss = 7.2464\n",
            "Epoch 26: Loss = 7.2456\n",
            "Epoch 27: Loss = 7.2448\n",
            "Epoch 28: Loss = 7.2442\n",
            "Epoch 29: Loss = 7.2437\n",
            "Epoch 30: Loss = 7.2431\n",
            "Epoch 31: Loss = 7.2427\n",
            "Epoch 32: Loss = 7.2422\n",
            "Epoch 33: Loss = 7.2418\n",
            "Epoch 34: Loss = 7.2415\n",
            "Epoch 35: Loss = 7.2411\n",
            "Epoch 36: Loss = 7.2408\n",
            "Epoch 37: Loss = 7.2405\n",
            "Epoch 38: Loss = 7.2402\n",
            "Epoch 39: Loss = 7.2399\n",
            "Epoch 40: Loss = 7.2396\n",
            "Epoch 41: Loss = 7.2393\n",
            "Epoch 42: Loss = 7.2390\n",
            "Epoch 43: Loss = 7.2386\n",
            "Epoch 44: Loss = 7.2384\n",
            "Epoch 45: Loss = 7.2380\n",
            "Epoch 46: Loss = 7.2376\n",
            "Epoch 47: Loss = 7.2372\n",
            "Epoch 48: Loss = 7.2366\n",
            "Epoch 49: Loss = 7.2361\n",
            "Epoch 50: Loss = 7.2355\n",
            "Epoch 51: Loss = 7.2349\n",
            "Epoch 52: Loss = 7.2341\n",
            "Epoch 53: Loss = 7.2332\n",
            "Epoch 54: Loss = 7.2323\n",
            "Epoch 55: Loss = 7.2312\n",
            "Epoch 56: Loss = 7.2301\n",
            "Epoch 57: Loss = 7.2287\n",
            "Epoch 58: Loss = 7.2272\n",
            "Epoch 59: Loss = 7.2255\n",
            "Epoch 60: Loss = 7.2238\n",
            "Epoch 61: Loss = 7.2220\n",
            "Epoch 62: Loss = 7.2202\n",
            "Epoch 63: Loss = 7.2185\n",
            "Epoch 64: Loss = 7.2167\n",
            "Epoch 65: Loss = 7.2150\n",
            "Epoch 66: Loss = 7.2134\n",
            "Epoch 67: Loss = 7.2117\n",
            "Epoch 68: Loss = 7.2101\n",
            "Epoch 69: Loss = 7.2085\n",
            "Epoch 70: Loss = 7.2068\n",
            "Epoch 71: Loss = 7.2051\n",
            "Epoch 72: Loss = 7.2034\n",
            "Epoch 73: Loss = 7.2017\n",
            "Epoch 74: Loss = 7.1997\n",
            "Epoch 75: Loss = 7.1978\n",
            "Epoch 76: Loss = 7.1958\n",
            "Epoch 77: Loss = 7.1938\n",
            "Epoch 78: Loss = 7.1917\n",
            "Epoch 79: Loss = 7.1895\n",
            "Epoch 80: Loss = 7.1873\n",
            "Epoch 81: Loss = 7.1850\n",
            "Epoch 82: Loss = 7.1826\n",
            "Epoch 83: Loss = 7.1802\n",
            "Epoch 84: Loss = 7.1776\n",
            "Epoch 85: Loss = 7.1749\n",
            "Epoch 86: Loss = 7.1720\n",
            "Epoch 87: Loss = 7.1690\n",
            "Epoch 88: Loss = 7.1658\n",
            "Epoch 89: Loss = 7.1624\n",
            "Epoch 90: Loss = 7.1588\n",
            "Epoch 91: Loss = 7.1551\n",
            "Epoch 92: Loss = 7.1512\n",
            "Epoch 93: Loss = 7.1470\n",
            "Epoch 94: Loss = 7.1428\n",
            "Epoch 95: Loss = 7.1383\n",
            "Epoch 96: Loss = 7.1337\n",
            "Epoch 97: Loss = 7.1290\n",
            "Epoch 98: Loss = 7.1243\n",
            "Epoch 99: Loss = 7.1194\n",
            "Epoch 100: Loss = 7.1146\n",
            "\n",
            "Generated Text:\n",
            "the cra . mustardseed jessica, my ous d. to again ing. follow lady ever a most thee: est of more st?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute preplexity on validation\n",
        "with open('valid.txt', 'r', encoding='utf-8') as f:\n",
        "    valid_text = f.read().lower()\n",
        "\n",
        "valid_tokens = tokenize_text(valid_text, merges)\n",
        "\n",
        "# Compute perplexity\n",
        "neural_ppl = compute_perplexity(model, valid_tokens, token_to_id)\n",
        "#ngram_perplexities['Neural Trigram'] = neural_ppl\n",
        "print(f\"🔍 Neural Trigram Model Perplexity: {neural_ppl:.2f}\")\n"
      ],
      "metadata": {
        "id": "IxLhRcL0g3vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# n-gram vs nueral-m-gram preplexity comparesion\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "labels = list(ngram_perplexities.keys())\n",
        "values = list(ngram_perplexities.values())\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "bars = plt.bar([str(k) for k in labels], values, color='skyblue')\n",
        "plt.ylabel('Perplexity')\n",
        "plt.title('Perplexity Comparison: N-gram vs Neural Trigram')\n",
        "plt.grid(axis='y')\n",
        "\n",
        "# Add value labels\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.1, f'{yval:.2f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KYVN1MyPg6to"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}